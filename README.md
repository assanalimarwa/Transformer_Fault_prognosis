# Transformer_Fault_prognosis

This repository contains the code for Transformer Fault Prognosis using a multi-head self-attention mechanism applied to bidirectional RNNs, specifically constructing MHSA-BiGRU and MHSA-BiLSTM models. These architectures combine the strengths of bidirectional RNNs for sequential data processing with the powerful contextual understanding provided by the multi-head self-attention mechanism. 

